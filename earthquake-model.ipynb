{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run Earthquake Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 27 22:13:42 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import lstm_estimator\n",
    "import earthquake_input_fn\n",
    "import lstm_estimator_4096\n",
    "from importlib import reload\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_tf():\n",
    "    global lstm_estimator\n",
    "    global earthquake_input_fn\n",
    "    global lstm_estimator_4096\n",
    "    for i in range(2):\n",
    "        import lstm_estimator\n",
    "        import earthquake_input_fn\n",
    "        import lstm_estimator_4096\n",
    "        reload(lstm_estimator)\n",
    "        reload(earthquake_input_fn)\n",
    "        reload(lstm_estimator_4096)\n",
    "        del lstm_estimator\n",
    "        del earthquake_input_fn\n",
    "        del lstm_estimator_4096\n",
    "    import lstm_estimator\n",
    "    import earthquake_input_fn\n",
    "    import lstm_estimator_4096\n",
    "reload_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_estimator_from_params(batch_size, timesteps, n_feats, feature_columns,\n",
    "#                                 lstm_cell_size, dense_size, learning_rate, dropout_rate=0.5, lambda_l2_reg=0.0005,\n",
    "#                                 label_input_column=None, model_dir=None, eval_every=600):\n",
    "#    \n",
    "#    params = {\n",
    "#        'batch_size': batch_size,\n",
    "#        'timesteps': timesteps,\n",
    "#        'n_feats': n_feats,\n",
    "#        'feature_columns': feature_columns,\n",
    "#        'lstm_cell_size': lstm_cell_size,\n",
    "#        'dense_size': dense_size,\n",
    "#        'learning_rate': learning_rate,\n",
    "#        'dropout_rate': dropout_rate,\n",
    "#        'lambda_l2_reg': lambda_l2_reg,\n",
    "#        'label_input_column': label_input_column\n",
    "#    }\n",
    "#    if model_dir is None:\n",
    "#        model_dir = '/workspace/persistent-data/models/%s' % (time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "#        \n",
    "#    config = tf.estimator.RunConfig(model_dir=model_dir,\n",
    "#                                    log_step_count_steps=int(4096 / timesteps),\n",
    "#                                    save_checkpoints_secs=600)\n",
    "#    \n",
    "#    estim = tf.estimator.Estimator(model_fn=lstm_estimator.lstm_model_fn,\n",
    "#                                   params=params,\n",
    "#                                   model_dir=model_dir,\n",
    "#                                   config=config)\n",
    "#    return estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator_4096_from_params(batch_size, timesteps, n_feats, feature_columns,\n",
    "                                 lstm_cell_size, dense_size, cnn_size, learning_rate, dropout_rate=0.5, time_pool=8, lambda_l2_reg=0.0005,\n",
    "                                 label_input_column=None, model_dir=None, eval_every=600):\n",
    "    \n",
    "    params = {\n",
    "        'batch_size': batch_size,\n",
    "        'timesteps': timesteps,\n",
    "        'n_feats': n_feats,\n",
    "        'feature_columns': feature_columns,\n",
    "        'lstm_cell_size': lstm_cell_size,\n",
    "        'dense_size': dense_size,\n",
    "        'cnn_size': cnn_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'lambda_l2_reg': lambda_l2_reg,\n",
    "        'label_input_column': label_input_column,\n",
    "        'time_pool': time_pool\n",
    "    }\n",
    "    if model_dir is None:\n",
    "        model_dir = '/workspace/persistent-data/models/%s' % (time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "    \n",
    "    params_file = osp.join(osp.split(model_dir)[0], osp.split(model_dir)[1] + '.params.pickle')\n",
    "    if not osp.isfile(params_file):\n",
    "        print('writing params to %s' % params_file)\n",
    "        with open(params_file, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        \n",
    "    config = tf.estimator.RunConfig(model_dir=model_dir,\n",
    "                                    log_step_count_steps=int(4096 / timesteps) * 20,\n",
    "                                    save_checkpoints_secs=600)\n",
    "    \n",
    "    estim = tf.estimator.Estimator(model_fn=lstm_estimator_4096.lstm_4096_model_fn,\n",
    "                                   params=params,\n",
    "                                   model_dir=model_dir,\n",
    "                                   config=config)\n",
    "    return estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_and_evaluate_estimator(earthquake_data_dir, scales, eval_every, eval_count, batch_size,\n",
    "#                                 timesteps, n_feats, feature_columns, lstm_cell_size, dense_size, learning_rate,\n",
    "#                                 dropout_rate, lambda_l2_reg, model_dir=None):\n",
    "#    \n",
    "#    estim = create_estimator_from_params(batch_size, timesteps, n_feats, feature_columns=feature_columns,\n",
    "#                                         lstm_cell_size=lstm_cell_size, dense_size=dense_size, learning_rate=learning_rate,\n",
    "#                                         dropout_rate=dropout_rate, lambda_l2_reg=lambda_l2_reg,\n",
    "#                                         label_input_column=None, model_dir=model_dir, eval_every=eval_every)\n",
    "#    \n",
    "#    trainspec = tf.estimator.TrainSpec(input_fn=lambda: earthquake_input_fn.earthquake_input_fn2(earthquake_data_dir,\n",
    "#                                                                             batch_size,\n",
    "#                                                                             timesteps,\n",
    "#                                                                             scales=scales,\n",
    "#                                                                             traintest='train'),\n",
    "#                                       max_steps=1000000)\n",
    "#    \n",
    "#    evalspec = tf.estimator.EvalSpec(input_fn=lambda: earthquake_input_fn.earthquake_input_fn2(earthquake_data_dir,\n",
    "#                                                                           batch_size,\n",
    "#                                                                           timesteps,\n",
    "#                                                                           scales=scales,\n",
    "#                                                                           traintest='test'),\n",
    "#                                     steps=int(eval_count * (4096/timesteps)),\n",
    "#                                     start_delay_secs=1, throttle_secs=eval_every)\n",
    "#    \n",
    "#    return estim, trainspec, evalspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_estimator_4096(earthquake_data_dir, scales, eval_every, eval_count, batch_size,\n",
    "                                      timesteps, n_feats, feature_columns, lstm_cell_size, dense_size, cnn_size,\n",
    "                                      learning_rate, dropout_rate, time_pool, lambda_l2_reg, model_dir=None):\n",
    "    \n",
    "    estim = create_estimator_4096_from_params(batch_size, timesteps, n_feats, feature_columns=feature_columns,\n",
    "                                              lstm_cell_size=lstm_cell_size, dense_size=dense_size, cnn_size=cnn_size,\n",
    "                                              learning_rate=learning_rate, dropout_rate=dropout_rate, time_pool=time_pool,\n",
    "                                              lambda_l2_reg=lambda_l2_reg, label_input_column=None, model_dir=model_dir,\n",
    "                                              eval_every=eval_every)\n",
    "    \n",
    "    trainspec = tf.estimator.TrainSpec(input_fn=lambda: earthquake_input_fn.earthquake_input_fn2(earthquake_data_dir,\n",
    "                                                                             batch_size,\n",
    "                                                                             timesteps,\n",
    "                                                                             scales=scales,\n",
    "                                                                             traintest='train'),\n",
    "                                       max_steps=1000000)\n",
    "    \n",
    "    evalspec = tf.estimator.EvalSpec(input_fn=lambda: earthquake_input_fn.earthquake_input_fn2(earthquake_data_dir,\n",
    "                                                                           batch_size,\n",
    "                                                                           timesteps,\n",
    "                                                                           scales=scales,\n",
    "                                                                           traintest='test'),\n",
    "                                     steps=int(eval_count * (4096/timesteps)),\n",
    "                                     start_delay_secs=1, throttle_secs=eval_every)\n",
    "    \n",
    "    return estim, trainspec, evalspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload_tf()\n",
    "#\n",
    "#EARTHQUAKE_DATA_DIR = '/workspace/persistent-data/earthquake/tfrecords2'\n",
    "#BATCH_SIZE=64\n",
    "#TIMESTEPS=4096\n",
    "#LSTM_CELL_SIZE=[64, 64]\n",
    "#DENSE_SIZE=[64, 64]\n",
    "#LEARNING_RATE=0.005\n",
    "#DROPOUT_RATE=0.5\n",
    "#LAMBDA_L2_REG=0.0005\n",
    "#STEPS_PER_BATCH = int(4096/TIMESTEPS)\n",
    "#EVAL_EVERY_N_SECONDS = 1800\n",
    "#EVAL_NUM_BATCHES = 100\n",
    "#N_FEATS = 1\n",
    "#SCALES = ['1e' + str(i) for i in range(-8,3)]\n",
    "#FEATURE_COLUMNS = [tf.feature_column.numeric_column(key='acousticdata', dtype=tf.int64, shape=(TIMESTEPS,))]\n",
    "#\n",
    "#estim, train_spec, eval_spec = train_and_evaluate_estimator(EARTHQUAKE_DATA_DIR, SCALES, EVAL_EVERY_N_SECONDS,\n",
    "#                                                            EVAL_NUM_BATCHES, BATCH_SIZE, TIMESTEPS, N_FEATS, FEATURE_COLUMNS,\n",
    "#                                                            LSTM_CELL_SIZE, DENSE_SIZE, LEARNING_RATE, DROPOUT_RATE, LAMBDA_L2_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_is_chief': True, '_device_fn': None, '_global_id_in_cluster': 0, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_save_checkpoints_secs': 600, '_eval_distribute': None, '_num_ps_replicas': 0, '_task_id': 0, '_keep_checkpoint_every_n_hours': 10000, '_experimental_distribute': None, '_master': '', '_tf_random_seed': None, '_task_type': 'worker', '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f55b8be7e10>, '_num_worker_replicas': 1, '_train_distribute': None, '_protocol': None, '_model_dir': '/workspace/persistent-data/models/2019-01-28-05-18-41', '_save_checkpoints_steps': None, '_evaluation_master': '', '_log_step_count_steps': 20, '_service': None}\n"
     ]
    }
   ],
   "source": [
    "reload_tf()\n",
    "\n",
    "EARTHQUAKE_DATA_DIR = '/workspace/persistent-data/earthquake/tfrecords2'\n",
    "BATCH_SIZE=32\n",
    "TIMESTEPS=4096\n",
    "LSTM_CELL_SIZE=[256, 256, 256]\n",
    "CNN_SIZE=[[32, 16], [32, 16], [32, 16], [32, 8], [32, 8]]\n",
    "DENSE_SIZE=[256, 256]\n",
    "LEARNING_RATE=0.00075\n",
    "DROPOUT_RATE=0.5\n",
    "TIME_POOL=16\n",
    "LAMBDA_L2_REG=0.005\n",
    "STEPS_PER_BATCH = int(4096/TIMESTEPS)\n",
    "EVAL_EVERY_N_SECONDS = 600\n",
    "EVAL_NUM_BATCHES = 100\n",
    "N_FEATS = 1\n",
    "SCALES = ['1e' + str(i) for i in range(-8,3)]\n",
    "FEATURE_COLUMNS = [tf.feature_column.numeric_column(key='acousticdata', dtype=tf.int64, shape=(TIMESTEPS,))]\n",
    "\n",
    "estim, train_spec, eval_spec = train_and_evaluate_estimator_4096(EARTHQUAKE_DATA_DIR, SCALES, EVAL_EVERY_N_SECONDS,\n",
    "                                                                 EVAL_NUM_BATCHES, BATCH_SIZE, TIMESTEPS, N_FEATS, FEATURE_COLUMNS,\n",
    "                                                                 LSTM_CELL_SIZE, DENSE_SIZE, CNN_SIZE, LEARNING_RATE, DROPOUT_RATE, TIME_POOL, LAMBDA_L2_REG,\n",
    "                                                                 model_dir='/workspace/persistent-data/models/2019-01-28-05-18-41'\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "FEATURES\n",
      "{'acousticdata': <tf.Tensor 'IteratorGetNext:0' shape=(?, ?) dtype=int64>}\n",
      "--------------------\n",
      "LABELS\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, ?), dtype=float64, device=/device:CPU:0)\n",
      "--------------------\n",
      "MODE\n",
      "train\n",
      "--------------------\n",
      "PARAMS\n",
      "{'timesteps': 4096, 'n_feats': 1, 'time_pool': 16, 'label_input_column': None, 'batch_size': 32, 'cnn_size': [[32, 16], [32, 16], [32, 16], [32, 8], [32, 8]], 'feature_columns': [_NumericColumn(key='acousticdata', shape=(4096,), default_value=None, dtype=tf.int64, normalizer_fn=None)], 'dense_size': [256, 256], 'lstm_cell_size': [256, 256, 256], 'dropout_rate': 0.5, 'lambda_l2_reg': 0.005, 'learning_rate': 0.00075}\n",
      "--------------------\n",
      "num_splits=256 (this is the number of timesteps fed to RNN)\n",
      "rnn_input Tensor(\"rnn_input/Sum:0\", shape=(32, 256, 1), dtype=float32)\n",
      "cnn_input Tensor(\"cnn_input/Reshape:0\", shape=(32, 4096, 1), dtype=float32)\n",
      "rnn_outputs Tensor(\"bi_rnn/cudnn_lstm/CudnnRNN:0\", shape=(32, 256, 512), dtype=float32)\n",
      "Tensor(\"cnn_structures/dropout_4/Identity:0\", shape=(32, 128, 32), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(32, 4096), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /workspace/persistent-data/models/2019-01-28-05-18-41/model.ckpt-3028\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3028 into /workspace/persistent-data/models/2019-01-28-05-18-41/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.004487396350016019, step = 3028\n",
      "INFO:tensorflow:global_step/sec: 1.83071\n",
      "INFO:tensorflow:loss = 0.0025517358201231047, step = 3048 (10.926 sec)\n"
     ]
    }
   ],
   "source": [
    "tf.estimator.train_and_evaluate(estim, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
