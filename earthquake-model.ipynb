{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run Earthquake Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb  4 14:31:02 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   51C    P0    33W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import lstm_estimator\n",
    "import earthquake_input_fn\n",
    "import lstm_estimator_4096\n",
    "from importlib import reload\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_tf():\n",
    "    global lstm_estimator\n",
    "    global earthquake_input_fn\n",
    "    global lstm_estimator_4096\n",
    "    for i in range(2):\n",
    "        import lstm_estimator\n",
    "        import earthquake_input_fn\n",
    "        import lstm_estimator_4096\n",
    "        reload(lstm_estimator)\n",
    "        reload(earthquake_input_fn)\n",
    "        reload(lstm_estimator_4096)\n",
    "        del lstm_estimator\n",
    "        del earthquake_input_fn\n",
    "        del lstm_estimator_4096\n",
    "    import lstm_estimator\n",
    "    import earthquake_input_fn\n",
    "    import lstm_estimator_4096\n",
    "reload_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator_4096_from_params(batch_size, timesteps, n_feats, feature_columns,\n",
    "                                      lstm_cell_size, dense_size, cnn_size, learning_rate, dropout_rate=0.5, grad_clip=0.5,\n",
    "                                      time_pool=8, lambda_l2_reg=0.0005, stft_frame_length=512, stft_frame_step=64,\n",
    "                                      label_input_column=None, model_dir=None, eval_every=600, use_stft=True, use_stride=True,\n",
    "                                      regularize_networks=True, use_dense_batch_norm=True, optimizer_name='Adam',\n",
    "                                      lstm_directionality='bidirectional'):\n",
    "    \n",
    "    params = {\n",
    "        'batch_size': batch_size,\n",
    "        'timesteps': timesteps,\n",
    "        'n_feats': n_feats,\n",
    "        'feature_columns': feature_columns,\n",
    "        'lstm_cell_size': lstm_cell_size,\n",
    "        'dense_size': dense_size,\n",
    "        'cnn_size': cnn_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'grad_clip': grad_clip,\n",
    "        'lambda_l2_reg': lambda_l2_reg,\n",
    "        'stft_frame_length': stft_frame_length,\n",
    "        'stft_frame_step': stft_frame_step,\n",
    "        'label_input_column': label_input_column,\n",
    "        'time_pool': time_pool,\n",
    "        'use_stft': use_stft,\n",
    "        'use_stride': use_stride,\n",
    "        'regularize_networks': regularize_networks,\n",
    "        'use_dense_batch_norm': use_dense_batch_norm,\n",
    "        'optimizer_name': optimizer_name,\n",
    "        'lstm_directionality': lstm_directionality\n",
    "    }\n",
    "    if model_dir is None:\n",
    "        model_dir = '/workspace/persistent-data/models/%s' % (time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "    \n",
    "    params_file = osp.join(osp.split(model_dir)[0], osp.split(model_dir)[1] + '.params.pickle')\n",
    "    if not osp.isfile(params_file):\n",
    "        print('writing params to %s' % params_file)\n",
    "        with open(params_file, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        \n",
    "    config = tf.estimator.RunConfig(model_dir=model_dir,\n",
    "                                    log_step_count_steps=int(500 / batch_size),\n",
    "                                    save_checkpoints_secs=300,\n",
    "                                    session_config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.6666)))\n",
    "    \n",
    "    estim = tf.estimator.Estimator(model_fn=lstm_estimator_4096.lstm_4096_model_fn,\n",
    "                                   params=params,\n",
    "                                   model_dir=model_dir,\n",
    "                                   config=config)\n",
    "    return estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_estimator_4096(earthquake_data_dir, eval_every_n_secs, eval_count, batch_size,\n",
    "                                      timesteps, n_feats, feature_columns, lstm_cell_size, dense_size, cnn_size,\n",
    "                                      learning_rate, dropout_rate, grad_clip, time_pool, lambda_l2_reg,\n",
    "                                      stft_frame_length, stft_frame_step, use_stft, use_stride, regularize_networks,\n",
    "                                      use_dense_batch_norm, optimizer_name, lstm_directionality, model_dir=None):\n",
    "    \n",
    "    estim = create_estimator_4096_from_params(batch_size, timesteps, n_feats, feature_columns=feature_columns,\n",
    "                                              lstm_cell_size=lstm_cell_size, dense_size=dense_size, cnn_size=cnn_size,\n",
    "                                              learning_rate=learning_rate, dropout_rate=dropout_rate, grad_clip=grad_clip,\n",
    "                                              time_pool=time_pool, lambda_l2_reg=lambda_l2_reg, stft_frame_length=stft_frame_length,\n",
    "                                              stft_frame_step=stft_frame_step, label_input_column=None, model_dir=model_dir,\n",
    "                                              eval_every=eval_every_n_secs, use_stft=use_stft, use_stride=use_stride,\n",
    "                                              regularize_networks=regularize_networks, use_dense_batch_norm=use_dense_batch_norm,\n",
    "                                              optimizer_name=optimizer_name, lstm_directionality=lstm_directionality)\n",
    "    \n",
    "    trainspec = tf.estimator.TrainSpec(input_fn=lambda: earthquake_input_fn.earthquake_input_fn2(earthquake_data_dir,\n",
    "                                                                             batch_size,\n",
    "                                                                             timesteps,\n",
    "                                                                             traintest='train'),\n",
    "                                       max_steps=1000000)\n",
    "    \n",
    "    evalspec = tf.estimator.EvalSpec(input_fn=lambda: earthquake_input_fn.earthquake_input_fn2(earthquake_data_dir,\n",
    "                                                                           batch_size,\n",
    "                                                                           timesteps,\n",
    "                                                                           traintest='test'),\n",
    "                                     steps=eval_count,\n",
    "                                     start_delay_secs=1, throttle_secs=eval_every_n_secs)\n",
    "    \n",
    "    return estim, trainspec, evalspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing params to /workspace/persistent-data/models/2019-02-04-14-31-11.params.pickle\n",
      "INFO:tensorflow:Using config: {'_is_chief': True, '_keep_checkpoint_max': 5, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2056738668>, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.6666\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_experimental_distribute': None, '_log_step_count_steps': 15, '_global_id_in_cluster': 0, '_master': '', '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_num_worker_replicas': 1, '_device_fn': None, '_model_dir': '/workspace/persistent-data/models/2019-02-04-14-31-11', '_train_distribute': None, '_evaluation_master': '', '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 300, '_protocol': None, '_task_id': 0, '_task_type': 'worker', '_eval_distribute': None}\n"
     ]
    }
   ],
   "source": [
    "reload_tf()\n",
    "\n",
    "EARTHQUAKE_DATA_DIR = '/workspace/persistent-data/earthquake/tfrecords3'\n",
    "BATCH_SIZE=32\n",
    "TIMESTEPS=75000\n",
    "LSTM_CELL_SIZE=[512, 512]\n",
    "CNN_SIZE=[[32, 4], [32, 4], [16, 6], [12, 6], [8, 8]]\n",
    "DENSE_SIZE=[512, 512, 512, 4]\n",
    "STFT_FRAME_LENGTH=1024\n",
    "STFT_FRAME_STEP=300\n",
    "LEARNING_RATE=0.0005\n",
    "DROPOUT_RATE=0.5\n",
    "GRAD_CLIP=1\n",
    "TIME_POOL=300\n",
    "LAMBDA_L2_REG=0.00005\n",
    "EVAL_EVERY_N_SECONDS = 600\n",
    "EVAL_NUM_BATCHES = 500\n",
    "N_FEATS = 1\n",
    "USE_STFT = True\n",
    "USE_STRIDE = True\n",
    "REGULARIZE_NETWORKS = False\n",
    "USE_DENSE_BATCH_NORM = False\n",
    "OPTIMIZER_NAME = 'SGD'\n",
    "LSTM_DIRECTIONALITY = 'unidirectional'\n",
    "FEATURE_COLUMNS = [tf.feature_column.numeric_column(key='acousticdata', dtype=tf.float32, shape=(TIMESTEPS,))]\n",
    "\n",
    "estim, train_spec, eval_spec = train_and_evaluate_estimator_4096(EARTHQUAKE_DATA_DIR, EVAL_EVERY_N_SECONDS, EVAL_NUM_BATCHES,\n",
    "                                                                 BATCH_SIZE, TIMESTEPS, N_FEATS, FEATURE_COLUMNS,\n",
    "                                                                 LSTM_CELL_SIZE, DENSE_SIZE, CNN_SIZE, LEARNING_RATE,\n",
    "                                                                 DROPOUT_RATE, GRAD_CLIP, TIME_POOL, LAMBDA_L2_REG,\n",
    "                                                                 STFT_FRAME_LENGTH, STFT_FRAME_STEP, USE_STFT, USE_STRIDE,\n",
    "                                                                 REGULARIZE_NETWORKS, USE_DENSE_BATCH_NORM, OPTIMIZER_NAME,\n",
    "                                                                 LSTM_DIRECTIONALITY,\n",
    "                                                                 #model_dir='/workspace/persistent-data/models/2019-02-04-04-38-07'\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- BEGIN EPOCH 0 ---------------\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "FEATURES\n",
      "{'acousticdata': <tf.Tensor 'IteratorGetNext:0' shape=(?, 75000) dtype=float32>}\n",
      "--------------------\n",
      "LABELS\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 75000), dtype=float64, device=/device:CPU:0)\n",
      "--------------------\n",
      "MODE\n",
      "train\n",
      "--------------------\n",
      "PARAMS\n",
      "{'lstm_cell_size': [512, 512], 'cnn_size': [[32, 4], [32, 4], [16, 6], [12, 6], [8, 8]], 'grad_clip': 1, 'use_stride': True, 'label_input_column': None, 'timesteps': 75000, 'use_dense_batch_norm': False, 'n_feats': 1, 'lstm_directionality': 'unidirectional', 'dropout_rate': 0.5, 'stft_frame_length': 1024, 'lambda_l2_reg': 5e-05, 'stft_frame_step': 300, 'batch_size': 32, 'dense_size': [512, 512, 512, 4], 'regularize_networks': False, 'use_stft': True, 'optimizer_name': 'SGD', 'feature_columns': [_NumericColumn(key='acousticdata', shape=(75000,), default_value=None, dtype=tf.float32, normalizer_fn=None)], 'time_pool': 300, 'learning_rate': 0.0005}\n",
      "--------------------\n",
      "num_splits=250 (this is the number of timesteps fed to RNN)\n",
      "stride_min Tensor(\"stride_rnn_input/Min:0\", shape=(?, 250), dtype=float64, device=/device:GPU:0)\n",
      "stride_max Tensor(\"stride_rnn_input/Max:0\", shape=(?, 250), dtype=float64, device=/device:GPU:0)\n",
      "stride_mean Tensor(\"stride_rnn_input/moments/Squeeze:0\", shape=(?, 250), dtype=float64, device=/device:GPU:0)\n",
      "stride_var Tensor(\"stride_rnn_input/moments/Squeeze_1:0\", shape=(?, 250), dtype=float64, device=/device:GPU:0)\n",
      "stride_rnn_input Tensor(\"stride_rnn_input/stack_1:0\", shape=(?, 250, 4), dtype=float64, device=/device:GPU:0)\n",
      "stride_rnn_input Tensor(\"stride_rnn_input/transpose:0\", shape=(250, ?, 4), dtype=float64, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    try:\n",
    "        print('---------------- BEGIN EPOCH %d ---------------' % i)\n",
    "        tf.estimator.train_and_evaluate(estim, train_spec, eval_spec)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    #except:\n",
    "    #    print('end of epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
