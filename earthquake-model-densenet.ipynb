{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run Earthquake Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  1 04:41:45 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import lstm_estimator\n",
    "import earthquake_input_fn\n",
    "import densenet\n",
    "from importlib import reload\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_tf():\n",
    "    global lstm_estimator\n",
    "    global earthquake_input_fn\n",
    "    global densenet\n",
    "    global hooks\n",
    "    for i in range(2):\n",
    "        import lstm_estimator\n",
    "        import earthquake_input_fn\n",
    "        import hooks\n",
    "        import densenet\n",
    "        reload(lstm_estimator)\n",
    "        reload(earthquake_input_fn)\n",
    "        reload(hooks)\n",
    "        reload(densenet)\n",
    "        del lstm_estimator\n",
    "        del earthquake_input_fn\n",
    "        del hooks\n",
    "        del densenet\n",
    "    import lstm_estimator\n",
    "    import earthquake_input_fn\n",
    "    import densenet\n",
    "    import hooks\n",
    "reload_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_densenet_from_params(batch_size, timesteps, time_pool, growth_rate, layer_sizes, dropout_rate, compression_theta,\n",
    "                                dense_size, feature_columns, optimizer_name, learning_rate, weight_decay, momentum, nesterov,\n",
    "                                lambda_l2_reg, grad_clip,\n",
    "                                model_dir=None):\n",
    "    \n",
    "    params = {\n",
    "        'batch_size': batch_size,\n",
    "        'timesteps': timesteps,\n",
    "        'time_pool': time_pool,\n",
    "        'growth_rate': growth_rate,\n",
    "        'layer_sizes': layer_sizes,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'compression_theta': compression_theta,\n",
    "        'dense_size': dense_size,\n",
    "        'feature_columns': feature_columns,\n",
    "        'optimizer_name': optimizer_name,\n",
    "        'learning_rate': learning_rate,\n",
    "        'weight_decay': weight_decay,\n",
    "        'momentum': momentum,\n",
    "        'nesterov': nesterov,\n",
    "        'lambda_l2_reg': lambda_l2_reg,\n",
    "        'grad_clip': grad_clip\n",
    "    }\n",
    "    \n",
    "    if model_dir is None:\n",
    "        model_dir = '/workspace/persistent-data/models/densenet-%s' % (time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "    \n",
    "    params_file = osp.join(osp.split(model_dir)[0], osp.split(model_dir)[1] + '.params.pickle')\n",
    "    if not osp.isfile(params_file):\n",
    "        print('writing params to %s' % params_file)\n",
    "        with open(params_file, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        \n",
    "    config = tf.estimator.RunConfig(model_dir=model_dir,\n",
    "                                    log_step_count_steps=int(500 / batch_size),\n",
    "                                    save_checkpoints_secs=300,\n",
    "                                    session_config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.95)))\n",
    "    \n",
    "    estim = tf.estimator.Estimator(model_fn=densenet.densenet_model_fn,\n",
    "                                   params=params,\n",
    "                                   model_dir=model_dir,\n",
    "                                   config=config)\n",
    "    return estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_densenet(batch_size, timesteps, time_pool, growth_rate, layer_sizes, dropout_rate, compression_theta,\n",
    "                                dense_size, feature_columns, optimizer_name, learning_rate, weight_decay, momentum, nesterov,\n",
    "                                lambda_l2_reg, grad_clip,\n",
    "                                noise,\n",
    "                                earthquake_data_dir,\n",
    "                                eval_count,\n",
    "                                eval_every_n_secs,\n",
    "                                epochs=1,\n",
    "                                model_dir=None):\n",
    "    \n",
    "    estim = create_densenet_from_params(batch_size=batch_size, timesteps=timesteps, time_pool=time_pool, \n",
    "                                        growth_rate=growth_rate, layer_sizes=layer_sizes, dropout_rate=dropout_rate,\n",
    "                                        compression_theta=compression_theta, dense_size=dense_size,\n",
    "                                        feature_columns=feature_columns, optimizer_name=optimizer_name,\n",
    "                                        learning_rate=learning_rate, weight_decay=weight_decay, momentum=momentum,\n",
    "                                        nesterov=nesterov, lambda_l2_reg=lambda_l2_reg, grad_clip=grad_clip,\n",
    "                                        model_dir=model_dir)\n",
    "    \n",
    "    trainspec = tf.estimator.TrainSpec(input_fn=lambda: earthquake_input_fn.earthquake_input_fn2(earthquake_data_dir,\n",
    "                                                                             batch_size,\n",
    "                                                                             timesteps,\n",
    "                                                                             noise,\n",
    "                                                                             traintest='train',\n",
    "                                                                             epochs=epochs),\n",
    "                                       max_steps=1000000)\n",
    "    \n",
    "    evalspec = tf.estimator.EvalSpec(input_fn=lambda: earthquake_input_fn.earthquake_input_fn2(earthquake_data_dir,\n",
    "                                                                           batch_size,\n",
    "                                                                           timesteps,\n",
    "                                                                           noise,\n",
    "                                                                           traintest='test'),\n",
    "                                     steps=eval_count,\n",
    "                                     start_delay_secs=eval_every_n_secs, throttle_secs=eval_every_n_secs)\n",
    "    \n",
    "    return estim, trainspec, evalspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing params to /workspace/persistent-data/models/densenet-2019-04-01-08-44-02.params.pickle\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_task_id': 0, '_log_step_count_steps': 7, '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f66b8851c18>, '_is_chief': True, '_device_fn': None, '_save_checkpoints_steps': None, '_model_dir': '/workspace/persistent-data/models/densenet-2019-04-01-08-44-02', '_eval_distribute': None, '_master': '', '_task_type': 'worker', '_save_checkpoints_secs': 300, '_experimental_distribute': None, '_train_distribute': None, '_save_summary_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      ", '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_service': None}\n"
     ]
    }
   ],
   "source": [
    "reload_tf()\n",
    "\n",
    "EARTHQUAKE_DATA_DIR = '/workspace/persistent-data/earthquake/tfrecords5'\n",
    "BATCH_SIZE = 64\n",
    "TIMESTEPS = 150000\n",
    "TIME_POOL = 125\n",
    "GROWTH_RATE = 12\n",
    "LAYER_SIZES = [6, 12, 32, 32]\n",
    "DROPOUT_RATE = 0.2\n",
    "COMPRESSION_THETA = 0.5\n",
    "DENSE_SIZE=128\n",
    "OPTIMIZER_NAME = 'MomentumW'\n",
    "LEARNING_RATE = 0.1\n",
    "WEIGHT_DECAY = 0.0001\n",
    "MOMENTUM = 0.9\n",
    "NESTEROV = True\n",
    "LAMBDA_L2_REG = 0.0001\n",
    "GRAD_CLIP = 1\n",
    "NOISE = 0.00\n",
    "EVAL_NUM_BATCHES = 100\n",
    "EVAL_EVERY_N_SECONDS = 1800\n",
    "EPOCHS = 10\n",
    "FEATURE_COLUMNS = [tf.feature_column.numeric_column(key='acousticdata', dtype=tf.float32, shape=TIMESTEPS)]\n",
    "\n",
    "estim, train_spec, eval_spec = train_and_evaluate_densenet(BATCH_SIZE, TIMESTEPS, TIME_POOL, GROWTH_RATE, LAYER_SIZES,\n",
    "                                                           DROPOUT_RATE, COMPRESSION_THETA, DENSE_SIZE, FEATURE_COLUMNS,\n",
    "                                                           OPTIMIZER_NAME, LEARNING_RATE, WEIGHT_DECAY, MOMENTUM,\n",
    "                                                           NESTEROV, LAMBDA_L2_REG, GRAD_CLIP,\n",
    "                                                           NOISE,\n",
    "                                                           EARTHQUAKE_DATA_DIR,\n",
    "                                                           EVAL_NUM_BATCHES,\n",
    "                                                           EVAL_EVERY_N_SECONDS,\n",
    "                                                           EPOCHS,\n",
    "                                                           # MODEL_DIR\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "{'learning_rate': 0.1, 'dropout_rate': 0.2, 'time_pool': 125, 'timesteps': 150000, 'dense_size': 128, 'compression_theta': 0.5, 'nesterov': True, 'growth_rate': 12, 'optimizer_name': 'MomentumW', 'grad_clip': 1, 'lambda_l2_reg': 0.0001, 'weight_decay': 0.0001, 'feature_columns': [_NumericColumn(key='acousticdata', shape=(150000,), default_value=None, dtype=tf.float32, normalizer_fn=None)], 'momentum': 0.9, 'batch_size': 64, 'layer_sizes': [6, 12, 32, 32]}\n",
      "num_splits=1200 (this is the number of timesteps fed to stride CNN)\n",
      "stft_input Tensor(\"input_ops/Cast_1:0\", shape=(?, 1200, 65), dtype=float64)\n",
      "stride_min Tensor(\"input_ops/Min:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_max Tensor(\"input_ops/Max:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_mean Tensor(\"input_ops/moments/Squeeze:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_var Tensor(\"input_ops/moments/Squeeze_1:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_pctiles Tensor(\"input_ops/concat:0\", shape=(?, 1200, 6), dtype=float64)\n",
      "stride_minroc Tensor(\"input_ops/Min_1:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_maxroc Tensor(\"input_ops/Max_1:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_meanroc Tensor(\"input_ops/moments_1/Squeeze:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_varroc Tensor(\"input_ops/moments_1/Squeeze_1:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_roc_pctiles Tensor(\"input_ops/concat_1:0\", shape=(?, 1200, 6), dtype=float64)\n",
      "stride_minroroc Tensor(\"input_ops/Min_2:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_maxroroc Tensor(\"input_ops/Max_2:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_meanroroc Tensor(\"input_ops/moments_2/Squeeze:0\", shape=(?, 1200), dtype=float64)\n",
      "stride_varroroc Tensor(\"input_ops/moments_2/Squeeze_1:0\", shape=(?, 1200), dtype=float64)\n",
      "stft Tensor(\"input_ops/Cast_1:0\", shape=(?, 1200, 65), dtype=float64)\n",
      "top_freq_indices Tensor(\"input_ops/TopKV2:1\", shape=(?, 1200, 3), dtype=int32)\n",
      "top_freq_embeddings Tensor(\"input_ops/Reshape_2:0\", shape=(?, 1200, 24), dtype=float64)\n",
      "results Tensor(\"output_dense/BiasAdd:0\", shape=(?, 1), dtype=float64, device=/device:GPU:0)\n",
      "labels Tensor(\"IteratorGetNext:1\", shape=(?, 150000), dtype=float64, device=/device:CPU:0)\n",
      "target Tensor(\"strided_slice:0\", shape=(?,), dtype=float64)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.estimator.train_and_evaluate(estim, train_spec, eval_spec)\n",
    "except KeyboardInterrupt as e:\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print('----------------------')\n",
    "    print('----------------------')\n",
    "    print('----------------------')\n",
    "    print('----------------------')\n",
    "    print('----------------------')\n",
    "    print('--  TRAINING ERROR  --')\n",
    "    print('----------------------')\n",
    "    print('----------------------')\n",
    "    print('----------------------')\n",
    "    print('----------------------')\n",
    "    print('----------------------')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
